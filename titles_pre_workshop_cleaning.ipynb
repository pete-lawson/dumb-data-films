{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a921149e",
   "metadata": {},
   "source": [
    "### Data preparation for the Unserious Data: JHU in Film session for Love Data Week 2026\n",
    "\n",
    "During this workshop, we will explore JHU in film by combining data from separate sources, including data on IMDB titles, data on IMDB ratings, and JHU titles, as described on the Hub article [Johns Hopkins on Film](https://hub.jhu.edu/2015/10/13/johns-hopkins-on-film/).\n",
    "\n",
    "\n",
    "Given the time limits of the workshop, we will provide a partially precleaned version of these data for the session, to make the data easier to work with. If you are interested in learning how the data was prepared from it's raw, original source, please follow along below! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a678f43d",
   "metadata": {},
   "source": [
    "### Scraping Film Titles from Hub Article\n",
    "The titles JHU appears in are listed on the Hub article [Johns Hopkins on Film](https://hub.jhu.edu/2015/10/13/johns-hopkins-on-film/). We would like to structure these neatly as tabular data, with a column for the title, and a column for the year it was produced. We could manually transcribe the titles â€“ there's not that many â€“ but what's the fun in that?\n",
    "\n",
    "Instead we can write a web scraper to extract the relevant information using the `Beautiful Soup` library. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b848999b",
   "metadata": {},
   "source": [
    "Let's import the libraries we need for scraping: `requests` to extract the HTML from the Hub article, `Beautiful Soup` to make sense of it, the regular expression library `re` to help pull out what we need, and `pandas` to format it as tabular data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbdab501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb052a",
   "metadata": {},
   "source": [
    "Now let's formulate our request that extracts the HTML from the Hub article. That HTML will be hard to work with, so we can use the `Beautiful Soup` html parser to turn it into a parse tree, which makes it (relatively) easy to navigate the nested information in that Hub article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c0f47ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://hub.jhu.edu/2015/10/13/johns-hopkins-on-film/')\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8005cbf4",
   "metadata": {},
   "source": [
    "When scraping, a good strategy is to identify any commonalities in the information you would like to extract. Let's take a look at the Hub article:\n",
    "![Hub article](images/hopkins-on-film.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06110471",
   "metadata": {},
   "source": [
    "We see that \"Filmed at Hopkins\" is larger than the rest of the text â€“ that's a heading. \n",
    "\n",
    "We then see that each of the titles are bolded. Nothing else on the page is bolded. This makes things easy! We just need to find the bolded text! We bold text in HTML using `<strong>I'm so bold!</strong>`, so to start, we need to find all the strong ðŸ’ª text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc75e58",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eab186",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_raw = soup.find_all('strong')\n",
    "titles_cleaned = []\n",
    "for title in titles_raw:\n",
    "    title_name_element = title.find('em')\n",
    "    title_name = title_name_element.getText(strip=True)\n",
    "    title_name_element.extract()\n",
    "    title_year_raw = title.get_text(strip=True)\n",
    "    title_year = re.search(r'(?<=\\()(\\d*)', title_year_raw).group(0)\n",
    "    titles_cleaned.append({'Title': title_name, 'Year': title_year})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fe2d3d",
   "metadata": {},
   "source": [
    "Now let's change that list to a `pandas` dataframe and save as a csv. This is the cleaned data you worked with during the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3c6eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_data = pd.DataFrame(titles_cleaned) \n",
    "titles_data.to_csv('data/cleaned/jhu-titles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a918a70",
   "metadata": {},
   "source": [
    "### Preparing IMDB Data\n",
    "Now it's time to tackle the raw IMDB data. Let's take a look at the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a666df2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--@ 1 plawson  staff   2.5G Nov 20 20:36 data/title.akas.tsv\r\n",
      "-rw-r--r--@ 1 plawson  staff   996M Nov 20 20:34 data/title.basics.tsv\r\n",
      "-rw-r--r--@ 1 plawson  staff   380M Nov 20 20:34 data/title.crew.tsv\r\n",
      "-rw-r--r--@ 1 plawson  staff   233M Nov 20 20:34 data/title.episode.tsv\r\n",
      "-rw-r--r--@ 1 plawson  staff   4.0G Nov 20 20:36 data/title.principals.tsv\r\n",
      "-rw-r--r--@ 1 plawson  staff    27M Nov 20 20:33 data/title.ratings.tsv\r\n"
     ]
    }
   ],
   "source": [
    "%ls -lh data/title.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334fc6bd",
   "metadata": {},
   "source": [
    "Yikes! Some of those files are pretty big! We could use `pandas` to load them in to a dataframe, but that's going to eat up a lot of memory. A smart way to work with the data would be to only load into memory exactly what we need, and nothing more. \n",
    "\n",
    "There is a great tool for doing this â€“ databases! And Python has a library that make creating and working with databases really easy â€“Â `duckdb`. \n",
    "\n",
    "> Fun fact: Hannes MÃ¼hleise, the creator of `duckdb`, named it for his pet duck Wilbur, who used to live on a boat with him. So if you get intimidated working with databases, just remember Wilbur, and you will feel better! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d2a9555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# open a duckdb database connection\n",
    "con = duckdb.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e248ef10",
   "metadata": {},
   "source": [
    "First, I want to return all the IMDB titles that match our JHU titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2a34da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu_titles_imdb = con.execute(\n",
    "    \"\"\"\n",
    "    SELECT tconst, titleType, primaryTitle, startYear, endYear, runtimeMinutes, genres\n",
    "    FROM read_csv_auto(\n",
    "    \"./data/title.basics.tsv\", \n",
    "    header=True,\n",
    "    nullstr=\"\\\\N\") as tb\n",
    "    JOIN titles_data as td ON tb.originalTitle = td.Title\n",
    "    WHERE tb.startYear = td.Year\n",
    "    \"\"\"\n",
    ").fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa72a2",
   "metadata": {},
   "source": [
    "It would be too easy if I gave you just the matches for our workshop, so I will generate a smaller, more maneagable set of data to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4258076",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_titles = con.execute(\n",
    "    \"\"\"\n",
    "    SELECT *\n",
    "    FROM (\n",
    "        SELECT tconst, titleType, primaryTitle, startYear, endYear, runtimeMinutes, genres\n",
    "        FROM read_csv_auto(\n",
    "        \"./data/title.basics.tsv\",\n",
    "        header=True,\n",
    "        nullstr=\"\\\\N\")\n",
    "    WHERE isAdult = 0)\n",
    "    USING SAMPLE 1000;\n",
    "    \"\"\"\n",
    ").fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed4ec4",
   "metadata": {},
   "source": [
    "Now let's combine our matches, and our random rows, and mix 'em up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27b92d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_title_basics_subset = pd.concat([random_titles, jhu_titles_imdb]).sample(frac=1, random_state=42)\n",
    "imdb_title_basics_subset.to_csv('data/cleaned/imdb-title-basics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2179f3",
   "metadata": {},
   "source": [
    "Finally, let's read in the ratings data, and save as a .csv for consistency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68c6c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_ratings = pd.read_csv('data/title.ratings.tsv', sep='\\t')\n",
    "imdb_ratings.to_csv('data/cleaned/imdb-ratings.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
